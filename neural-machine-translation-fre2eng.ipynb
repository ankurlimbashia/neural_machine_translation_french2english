{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:33:24.128977Z",
     "iopub.status.busy": "2021-01-21T13:33:24.128186Z",
     "iopub.status.idle": "2021-01-21T13:33:26.674139Z",
     "shell.execute_reply": "2021-01-21T13:33:26.673606Z"
    },
    "papermill": {
     "duration": 2.55919,
     "end_time": "2021-01-21T13:33:26.674245",
     "exception": false,
     "start_time": "2021-01-21T13:33:24.115055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-01-21 13:33:24--  http://www.manythings.org/anki/fra-eng.zip\r\n",
      "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3036::ac43:adc6, ...\r\n",
      "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 6129887 (5.8M) [application/zip]\r\n",
      "Saving to: ‘fra-eng.zip’\r\n",
      "\r\n",
      "fra-eng.zip         100%[===================>]   5.85M  5.13MB/s    in 1.1s    \r\n",
      "\r\n",
      "2021-01-21 13:33:26 (5.13 MB/s) - ‘fra-eng.zip’ saved [6129887/6129887]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:33:26.699157Z",
     "iopub.status.busy": "2021-01-21T13:33:26.698393Z",
     "iopub.status.idle": "2021-01-21T13:33:27.548511Z",
     "shell.execute_reply": "2021-01-21T13:33:27.548061Z"
    },
    "papermill": {
     "duration": 0.864008,
     "end_time": "2021-01-21T13:33:27.548610",
     "exception": false,
     "start_time": "2021-01-21T13:33:26.684602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ./fra-eng.zip\r\n",
      "  inflating: _about.txt              \r\n",
      "  inflating: fra.txt                 \r\n"
     ]
    }
   ],
   "source": [
    "!unzip ./fra-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:33:27.576443Z",
     "iopub.status.busy": "2021-01-21T13:33:27.575716Z",
     "iopub.status.idle": "2021-01-21T13:33:33.308759Z",
     "shell.execute_reply": "2021-01-21T13:33:33.307768Z"
    },
    "papermill": {
     "duration": 5.749362,
     "end_time": "2021-01-21T13:33:33.308907",
     "exception": false,
     "start_time": "2021-01-21T13:33:27.559545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Input,Embedding,TimeDistributed,RepeatVector\n",
    "from nltk.translate.bleu_score import SmoothingFunction,corpus_bleu\n",
    "smoothie = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:33:33.345280Z",
     "iopub.status.busy": "2021-01-21T13:33:33.343279Z",
     "iopub.status.idle": "2021-01-21T13:33:33.621266Z",
     "shell.execute_reply": "2021-01-21T13:33:33.620256Z"
    },
    "papermill": {
     "duration": 0.301099,
     "end_time": "2021-01-21T13:33:33.621391",
     "exception": false,
     "start_time": "2021-01-21T13:33:33.320292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = './fra.txt' # path of the file\n",
    "num_sentences = 20000 # no of sentences from the dataset that we are going to use\n",
    "\n",
    "# opening the text file and getting the data \n",
    "with open(data_path,'r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "    \n",
    "c=0 # to count the number of sentences\n",
    "\n",
    "# data cleaning\n",
    "source_texts,target_texts = [],[]\n",
    "for line in lines: # going through each lines\n",
    "    if c == num_sentences: # if we have 20000 sentences than we will get out of this loop\n",
    "        break \n",
    "    elif '\\t' in line:\n",
    "        op_data,ip_data,_ = line.lower().rstrip().split('\\t') # lowering the data and then spliting the data\n",
    "        \n",
    "        # to remove the punctuation we did not include last character\n",
    "        source_text = ip_data[:-1].strip()\n",
    "        target_text = op_data[:-1].strip()\n",
    "        # removing the unprintable character\n",
    "        # for english and french we will take anly alphabets of brespective languages and numbers\n",
    "        target_text = re.sub(\"[^a-z 1-9\\'-]\",\"\",target_text) \n",
    "        source_text = re.sub(\"[^a-zàâãçéèêëîïôœùûüÿ 1-9\\'-]\",\"\",source_text) \n",
    "        \n",
    "        source_texts.append(source_text)\n",
    "        target_texts.append(target_text)\n",
    "        c+=1\n",
    "\n",
    "# train_test_split of the source and target data\n",
    "source_train,source_test,target_train,target_test = train_test_split(source_texts,target_texts,test_size = 0.2, random_state= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the required functions for the data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:33:33.652187Z",
     "iopub.status.busy": "2021-01-21T13:33:33.650331Z",
     "iopub.status.idle": "2021-01-21T13:33:33.652845Z",
     "shell.execute_reply": "2021-01-21T13:33:33.653254Z"
    },
    "papermill": {
     "duration": 0.021365,
     "end_time": "2021-01-21T13:33:33.653354",
     "exception": false,
     "start_time": "2021-01-21T13:33:33.631989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenizer for data\n",
    "def create_tokenizer(texts):\n",
    "    tokenizer = Tokenizer(oov_token='<UNK>')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "# one_hot encoding of the target data\n",
    "def one_hot(pad_seq,max_sent_length,num_vocab):\n",
    "    target_data_one_hot = np.zeros((len(pad_seq),max_sent_length,num_vocab))\n",
    "    for i,w in enumerate(pad_seq):\n",
    "        for j,d in enumerate(w):\n",
    "            target_data_one_hot[i,j,d] = 1\n",
    "    return target_data_one_hot\n",
    "\n",
    "# for padding the data\n",
    "def encoding_text(tokenizer,text,max_length):\n",
    "    text_seq = tokenizer.texts_to_sequences(text)\n",
    "    pad_seq = pad_sequences(text_seq,maxlen= max_length)\n",
    "    return pad_seq\n",
    "\n",
    "# to find the maximum length of the sentence from data\n",
    "def max_length(text):\n",
    "    return max(len(l.split()) for l in text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:33:33.696956Z",
     "iopub.status.busy": "2021-01-21T13:33:33.691743Z",
     "iopub.status.idle": "2021-01-21T13:33:34.882871Z",
     "shell.execute_reply": "2021-01-21T13:33:34.883337Z"
    },
    "papermill": {
     "duration": 1.219756,
     "end_time": "2021-01-21T13:33:34.883461",
     "exception": false,
     "start_time": "2021-01-21T13:33:33.663705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5989 3227 11 5\n"
     ]
    }
   ],
   "source": [
    "# preparing source tokenizer and getting relevant information\n",
    "source_tokenizer = create_tokenizer(source_train)\n",
    "source_vocab = source_tokenizer.word_index\n",
    "num_source_vocab = len(source_vocab)+1\n",
    "max_source_length = max_length(source_train)\n",
    "\n",
    "# preparing target tokenizer and getting relevant information\n",
    "target_tokenizer = create_tokenizer(target_train)\n",
    "target_vocab = target_tokenizer.word_index\n",
    "num_target_vocab = len(target_vocab)+1\n",
    "max_target_length = max_length(target_train)\n",
    "\n",
    "# preparing the training data\n",
    "source_train_seq_pad = encoding_text(source_tokenizer,source_train,max_source_length) # padding of the source sentences\n",
    "target_train_seq_pad = encoding_text(target_tokenizer,target_train,max_target_length) # padding of the target sentences\n",
    "target_train_seq_pad = one_hot(target_train_seq_pad,max_target_length,num_target_vocab) # one hot encoding of the padded target senteces\n",
    "\n",
    "# preparing the test data\n",
    "source_test_seq_pad = encoding_text(source_tokenizer,source_test,max_source_length) # padding of the source sentences\n",
    "target_test_seq_pad = encoding_text(target_tokenizer,target_test,max_target_length) # padding of the target sentences\n",
    "target_test_seq_pad = one_hot(target_test_seq_pad,max_target_length,num_target_vocab) # one hot encoding of the padded target senteces\n",
    " \n",
    "print(num_source_vocab,num_target_vocab,max_source_length,max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing and running the Autoencoder model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:33:34.914566Z",
     "iopub.status.busy": "2021-01-21T13:33:34.914052Z",
     "iopub.status.idle": "2021-01-21T13:36:55.385009Z",
     "shell.execute_reply": "2021-01-21T13:36:55.384379Z"
    },
    "papermill": {
     "duration": 200.490724,
     "end_time": "2021-01-21T13:36:55.385175",
     "exception": false,
     "start_time": "2021-01-21T13:33:34.894451",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 11, 512)           3066368   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 5, 512)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 5, 3227)           1655451   \n",
      "=================================================================\n",
      "Total params: 8,920,219\n",
      "Trainable params: 8,920,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 3.9097 - acc: 0.3958\n",
      "Epoch 00001: val_acc improved from -inf to 0.41795, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 10s 38ms/step - loss: 3.9097 - acc: 0.3958 - val_loss: 3.5322 - val_acc: 0.4180\n",
      "Epoch 2/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 3.2263 - acc: 0.4516\n",
      "Epoch 00002: val_acc improved from 0.41795 to 0.48225, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 3.2259 - acc: 0.4516 - val_loss: 3.0426 - val_acc: 0.4823\n",
      "Epoch 3/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 2.7522 - acc: 0.5117\n",
      "Epoch 00003: val_acc improved from 0.48225 to 0.53450, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 2.7508 - acc: 0.5119 - val_loss: 2.6707 - val_acc: 0.5345\n",
      "Epoch 4/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 2.4231 - acc: 0.5534\n",
      "Epoch 00004: val_acc improved from 0.53450 to 0.56160, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 2.4232 - acc: 0.5535 - val_loss: 2.4705 - val_acc: 0.5616\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 2.1555 - acc: 0.5900\n",
      "Epoch 00005: val_acc improved from 0.56160 to 0.58855, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 33ms/step - loss: 2.1555 - acc: 0.5900 - val_loss: 2.3277 - val_acc: 0.5885\n",
      "Epoch 6/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 1.9258 - acc: 0.6266\n",
      "Epoch 00006: val_acc improved from 0.58855 to 0.60765, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 1.9256 - acc: 0.6266 - val_loss: 2.2052 - val_acc: 0.6076\n",
      "Epoch 7/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 1.7244 - acc: 0.6598\n",
      "Epoch 00007: val_acc improved from 0.60765 to 0.62155, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 1.7248 - acc: 0.6599 - val_loss: 2.1215 - val_acc: 0.6216\n",
      "Epoch 8/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 1.5349 - acc: 0.6919\n",
      "Epoch 00008: val_acc improved from 0.62155 to 0.63765, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 31ms/step - loss: 1.5352 - acc: 0.6919 - val_loss: 2.0458 - val_acc: 0.6377\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 1.3771 - acc: 0.7206\n",
      "Epoch 00009: val_acc improved from 0.63765 to 0.64860, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 1.3771 - acc: 0.7206 - val_loss: 1.9965 - val_acc: 0.6486\n",
      "Epoch 10/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 1.2264 - acc: 0.7502\n",
      "Epoch 00010: val_acc improved from 0.64860 to 0.65510, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 31ms/step - loss: 1.2269 - acc: 0.7501 - val_loss: 1.9462 - val_acc: 0.6551\n",
      "Epoch 11/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 1.0865 - acc: 0.7769\n",
      "Epoch 00011: val_acc improved from 0.65510 to 0.66745, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 33ms/step - loss: 1.0864 - acc: 0.7768 - val_loss: 1.8976 - val_acc: 0.6675\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.9705 - acc: 0.8011\n",
      "Epoch 00012: val_acc improved from 0.66745 to 0.67790, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 34ms/step - loss: 0.9705 - acc: 0.8011 - val_loss: 1.8809 - val_acc: 0.6779\n",
      "Epoch 13/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.8595 - acc: 0.8225\n",
      "Epoch 00013: val_acc improved from 0.67790 to 0.68400, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 33ms/step - loss: 0.8602 - acc: 0.8224 - val_loss: 1.8639 - val_acc: 0.6840\n",
      "Epoch 14/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.7678 - acc: 0.8412\n",
      "Epoch 00014: val_acc improved from 0.68400 to 0.68890, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 0.7686 - acc: 0.8410 - val_loss: 1.8475 - val_acc: 0.6889\n",
      "Epoch 15/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6854 - acc: 0.8573\n",
      "Epoch 00015: val_acc did not improve from 0.68890\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 0.6854 - acc: 0.8573 - val_loss: 1.8520 - val_acc: 0.6884\n",
      "Epoch 16/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.6099 - acc: 0.8742\n",
      "Epoch 00016: val_acc improved from 0.68890 to 0.69770, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 0.6099 - acc: 0.8742 - val_loss: 1.8548 - val_acc: 0.6977\n",
      "Epoch 17/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.5503 - acc: 0.8868\n",
      "Epoch 00017: val_acc improved from 0.69770 to 0.69785, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 0.5499 - acc: 0.8868 - val_loss: 1.8656 - val_acc: 0.6978\n",
      "Epoch 18/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.4984 - acc: 0.8975\n",
      "Epoch 00018: val_acc improved from 0.69785 to 0.70120, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 0.4990 - acc: 0.8974 - val_loss: 1.8578 - val_acc: 0.7012\n",
      "Epoch 19/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4536 - acc: 0.9052\n",
      "Epoch 00019: val_acc improved from 0.70120 to 0.70255, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 8s 33ms/step - loss: 0.4536 - acc: 0.9052 - val_loss: 1.8653 - val_acc: 0.7025\n",
      "Epoch 20/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.4156 - acc: 0.9141\n",
      "Epoch 00020: val_acc improved from 0.70255 to 0.70895, saving model to ./fre2eng.h5\n",
      "250/250 [==============================] - 9s 35ms/step - loss: 0.4156 - acc: 0.9141 - val_loss: 1.8518 - val_acc: 0.7089\n",
      "Epoch 21/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3796 - acc: 0.9197\n",
      "Epoch 00021: val_acc did not improve from 0.70895\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 0.3796 - acc: 0.9197 - val_loss: 1.8641 - val_acc: 0.7063\n",
      "Epoch 22/50\n",
      "249/250 [============================>.] - ETA: 0s - loss: 0.3507 - acc: 0.9241\n",
      "Epoch 00022: val_acc did not improve from 0.70895\n",
      "250/250 [==============================] - 8s 31ms/step - loss: 0.3513 - acc: 0.9240 - val_loss: 1.8859 - val_acc: 0.7064\n",
      "Epoch 23/50\n",
      "250/250 [==============================] - ETA: 0s - loss: 0.3258 - acc: 0.9285\n",
      "Epoch 00023: val_acc did not improve from 0.70895\n",
      "250/250 [==============================] - 8s 32ms/step - loss: 0.3258 - acc: 0.9285 - val_loss: 1.8720 - val_acc: 0.7081\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(max_source_length,)))\n",
    "model.add(Embedding(num_source_vocab,512,mask_zero=True))\n",
    "model.add(LSTM(512,return_sequences = False))\n",
    "model.add(RepeatVector(max_target_length))\n",
    "model.add(LSTM(512,return_sequences = True))\n",
    "model.add(TimeDistributed(Dense(num_target_vocab,activation = 'softmax')))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc',patience= 5,min_delta=0.01) # EarlyStoping callback to stop the fitting before all epochs\n",
    "filepath = './fre2eng.h5' # filepath required for checkpoint\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') # ModelCheckPoint to save the best model\n",
    "\n",
    "history = model.fit(source_train_seq_pad, target_train_seq_pad, \n",
    "                    epochs= 50,\n",
    "                    batch_size=64, \n",
    "                    validation_data = (source_test_seq_pad,target_test_seq_pad), \n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpoint,es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:36:57.032153Z",
     "iopub.status.busy": "2021-01-21T13:36:57.031063Z",
     "iopub.status.idle": "2021-01-21T13:36:57.047282Z",
     "shell.execute_reply": "2021-01-21T13:36:57.046676Z"
    },
    "papermill": {
     "duration": 0.844983,
     "end_time": "2021-01-21T13:36:57.047395",
     "exception": false,
     "start_time": "2021-01-21T13:36:56.202412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the weights from the best saved model\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the functions to predict the sequence and BLEU_sccore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:36:58.701542Z",
     "iopub.status.busy": "2021-01-21T13:36:58.699715Z",
     "iopub.status.idle": "2021-01-21T13:36:58.702231Z",
     "shell.execute_reply": "2021-01-21T13:36:58.702636Z"
    },
    "papermill": {
     "duration": 0.838898,
     "end_time": "2021-01-21T13:36:58.702743",
     "exception": false,
     "start_time": "2021-01-21T13:36:57.863845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a dictionary having key is a token number for a particular word and value is a word\n",
    "# this will required to decode the predicted sequence\n",
    "target_vocab_idx = {v:k for k,v in target_tokenizer.word_index.items()}\n",
    "\n",
    "# function to predict the decoded sequence\n",
    "def predict_sequence(model,sent,vocab_idx):\n",
    "    prediction = model.predict(sent.reshape(1,max_source_length))[0]\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    target = []\n",
    "    for i in integers:\n",
    "        if i != 0:\n",
    "            word = vocab_idx[i]\n",
    "            if word is None:\n",
    "                break\n",
    "            target.append(word)\n",
    "            \n",
    "    return ' '.join(target)\n",
    "\n",
    "# for evaluation of the model through BLEU_score\n",
    "def bleu_score(model,ip,ip_raw,op_raw,vocab_idx):\n",
    "    \n",
    "    prediction,actual = [],[]\n",
    "    for i,sent in enumerate(ip):\n",
    "        \n",
    "        if i%10 == 0: # to print the progress\n",
    "            print('\\rprogress ',(i+1)*100//len(ip),'%',sep='',end='',flush = True)\n",
    "        \n",
    "        translation = predict_sequence(model,sent,vocab_idx)\n",
    "        \n",
    "        prediction.append(translation)\n",
    "        actual.append(op_raw[i])\n",
    "    \n",
    "    print()\n",
    "    # printing the first ten sentences\n",
    "    for i in range(10):\n",
    "        print('French_sentence -',ip_raw[i],' | ',\n",
    "            'English_actual_sentence -',op_raw[i],' | ',\n",
    "            'English_predicted_sentence -',prediction[i])\n",
    "    \n",
    "    print()\n",
    "    # printing the BLEU_score\n",
    "    print('BLEU_SCORE')\n",
    "    print('BLEU score-1: %f' % corpus_bleu(actual, prediction, weights=(1.0, 0, 0, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "    print('BLEU score-2: %f' % corpus_bleu(actual, prediction, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "    print('BLEU score-3: %f' % corpus_bleu(actual, prediction, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie,auto_reweigh=False))\n",
    "    print('BLEU score-4: %f' % corpus_bleu(actual, prediction, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie,auto_reweigh=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:37:00.637318Z",
     "iopub.status.busy": "2021-01-21T13:37:00.636492Z",
     "iopub.status.idle": "2021-01-21T13:47:02.656690Z",
     "shell.execute_reply": "2021-01-21T13:47:02.656212Z"
    },
    "papermill": {
     "duration": 603.117053,
     "end_time": "2021-01-21T13:47:02.656787",
     "exception": false,
     "start_time": "2021-01-21T13:36:59.539734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress 99%\n",
      "French_sentence - nous savons tout cela  |  English_actual_sentence - we know all this  |  English_predicted_sentence - we know all this\n",
      "French_sentence - tu n'as pas de vie  |  English_actual_sentence - you have no life  |  English_predicted_sentence - you have no life\n",
      "French_sentence - je me suis senti triste  |  English_actual_sentence - i felt sad  |  English_predicted_sentence - i felt sad\n",
      "French_sentence - donnez-moi le vin  |  English_actual_sentence - give me the wine  |  English_predicted_sentence - give me the wine\n",
      "French_sentence - nous sommes en train d'attendre  |  English_actual_sentence - we're waiting  |  English_predicted_sentence - we're talking\n",
      "French_sentence - ce n'est pas équitable  |  English_actual_sentence - it's not fair  |  English_predicted_sentence - it's not fair\n",
      "French_sentence - c'est un vieux de la vieille  |  English_actual_sentence - he's an old timer  |  English_predicted_sentence - he's an old injured\n",
      "French_sentence - je l'apprécie  |  English_actual_sentence - i appreciate this  |  English_predicted_sentence - i appreciate this\n",
      "French_sentence - laisse-moi essayer à nouveau  |  English_actual_sentence - let me try again  |  English_predicted_sentence - let me try again\n",
      "French_sentence - j'ai été invité  |  English_actual_sentence - i was invited  |  English_predicted_sentence - i was invited\n",
      "\n",
      "BLEU_SCORE\n",
      "BLEU score-1: 0.689880\n",
      "BLEU score-2: 0.472877\n",
      "BLEU score-3: 0.418298\n",
      "BLEU score-4: 0.322103\n"
     ]
    }
   ],
   "source": [
    "bleu_score(model,source_train_seq_pad,source_train,target_train,target_vocab_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-21T13:47:05.406595Z",
     "iopub.status.busy": "2021-01-21T13:47:05.405722Z",
     "iopub.status.idle": "2021-01-21T13:49:35.405960Z",
     "shell.execute_reply": "2021-01-21T13:49:35.405286Z"
    },
    "papermill": {
     "duration": 151.514089,
     "end_time": "2021-01-21T13:49:35.406089",
     "exception": false,
     "start_time": "2021-01-21T13:47:03.892000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress 99%\n",
      "French_sentence - ne virez personne  |  English_actual_sentence - don't fire anyone  |  English_predicted_sentence - no anyone\n",
      "French_sentence - qui l'a bâti  |  English_actual_sentence - who built it  |  English_predicted_sentence - who built it\n",
      "French_sentence - puis-je revenir  |  English_actual_sentence - may i come again  |  English_predicted_sentence - can i i out\n",
      "French_sentence - ne répondez pas à cela  |  English_actual_sentence - don't answer that  |  English_predicted_sentence - don't answer that\n",
      "French_sentence - ça a marché  |  English_actual_sentence - did that work  |  English_predicted_sentence - it worked\n",
      "French_sentence - ferme-la  |  English_actual_sentence - shut up  |  English_predicted_sentence - shut shut up\n",
      "French_sentence - nous y sommes  |  English_actual_sentence - we are here  |  English_predicted_sentence - we everyone\n",
      "French_sentence - je dispose de l'immunité  |  English_actual_sentence - i have immunity  |  English_predicted_sentence - i have time\n",
      "French_sentence - nous sommes différents  |  English_actual_sentence - we're different  |  English_predicted_sentence - we're shaken\n",
      "French_sentence - c'est drôle  |  English_actual_sentence - it's funny  |  English_predicted_sentence - that's funny\n",
      "\n",
      "BLEU_SCORE\n",
      "BLEU score-1: 0.582607\n",
      "BLEU score-2: 0.448634\n",
      "BLEU score-3: 0.411146\n",
      "BLEU score-4: 0.320553\n"
     ]
    }
   ],
   "source": [
    "bleu_score(model,source_test_seq_pad,source_test,target_test,target_vocab_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 978.415403,
   "end_time": "2021-01-21T13:49:38.569102",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-21T13:33:20.153699",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
